example for [awk](http://cecileane.github.io/computingtools/pages/notes1013.html)

## problem

The problem was raised by a researcher
whose data were generated by a DNA sequencing method used by many other researchers.
This method returns data on thousands of genetic markers,
but is prone to a high fraction of missing data (after filtering
out low-confidence data). Data on one marker = one alignment file.
The pipeline to create alignment files, given some tuning parameter
for the filtering of low-confidence data, retains the missing data.
For some downstream analysis, this is undesirable: the researcher
needed a way to strip off the sequences made of missing values.

## data

in [`data/`](data): example files from 3 markers, that is, 3 alignment files,
in phylip format:

     number_of_samples number_of_sites_in_the_alignment
    sample_name_1 sequence_for_sample_1
    sample_name_2 sequence_for_sample_2
    ...

A missing sequence is a sequence made up entirely of the following symbols:
`?` or `-` or `N` or any combination of them.


## task 1

Write a script to create a new folder, `data_nomissing/`, with
one file for each file in `data/`, keeping the same file names.
The new files should be obtained from the original files by removing
the samples with a missing sequence.
The script should do so for all alignment files in a given folder
(here all alignment files in the `data/` folder).

example input:

     3 4
    sample1 AG-T
    sample2 ?--?
    sample3 A-CT

output file (in a new directory but with same file name as original file name):

     2 4
    sample1 AG-T
    sample3 A-CT

Technical requirements: write a **shell** script using a `for` loop
to go over all alignment files in the `data/` folder. You may assume that
the alignment files are all those ending in `.phy` in that folder.
Within this loop, use **`awk`** to handle each individual alignment file
(as well as some other commands).

To help you develop the script, the researcher provided only 3 example files.
Keep in mind, however, that your script will be used by this researcher on
thousands of files, and that this researcher might need to repeat the
analysis (including your script) for several values of their quality-vs-quantity
tuning parameter to filter out low-confidence data,
and that your script could also be used by other
researchers facing the same problem.
In short: you are motivated to write an efficient script!

## task 2

Write a second script to compare the input and output directories,
to check what happened. This script will:
1. create a csv-formatted file containing one row for
  each alignment: `alignment_filename,n_before,n_after` where
  `n` is the number of samples in each individual alignment file
2. write to the screen the following information:
  * number of alignments with no change
  * max number of deletions (i.e. max value of `n_before - n_after`)
  * total number of deletions (i.e. sum of all values of `n_before - n_after`)

For part 2, use a one-liner that reads the csv file from part 1, and that uses `awk`.
